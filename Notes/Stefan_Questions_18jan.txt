-Frank checken of goed op weg is met zijn eerste deel

-In tasks such as paraphrase detection, the model
is naturally required to carefully go through the
entire sentence to make a decision and thereby re-
sulting in delayed decision flips. In the QA task,
the attention ranking in the vanilla LSTM model
itself achieves a quick decision flip. On further in-
spection, we found that this is because these models
tend to attend onto answer words which are usually
a span in the input passage. So, when the repre-
sentations corresponding to the answer words are
erased, the model can no longer accurately predict
the answer resulting in a decision flip.

-bespreken extensions: 
  -*ortho Qfix* en wat we verwachten van experiment reruns daarmee (hoe kan het dat performance instort op SST, maar niet op andere taken als je ortho in Qencoder gebruikt?)
  -LIME, 
  -andere attentions, 
  -andere base models (BiLSTM: hidden state combineren of gewoon concat? Als concat hoe om te gaan met hogere aantal dims)


-how to reproduce Random Conicity Table 2? (Yun's fisrt attempt)
